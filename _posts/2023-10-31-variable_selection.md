# Thoughts on Variable Seletion  

The fact that there are a number of variable selection techniques available to the statistician underscores the general challenge variable selection poses. No one method is more effective or superior than any other method. Each has its own computational theories on which selection is based and each has its own weaknesses.  

Primarily what is at stake is the trade-off between bias and variance. A good variable selection model would have low bias - a number of uncorrelated variables that are used in the prediction model. Deciding what this number should be is the whole idea behind Variable Selection. If the number is too high, say for example all the variables, then bias would be appropriately low (since all the variables are being used to determine the prediction). However, the variance in such a model would be very high since it's most likely that some variables are correlated, negatively affecting the model. Conversely, if the number of variables is too small, then that would substantially increase our bias - the prediction model would be too dependent on a few (or single) variable.  

So the primary task of the statistician is to find the best "balance" between bias and variance. This is going to vary depending on both the data and the overriding question the data is being asked to answer.  Ideally, the best solution would be to fit a number of differing models on a training set and compare the results on the test set. When comparing the results, keep in mind the resources and time each model allocates - the model with the best results may be very resource intensive and only perform slightly better than a less resource intensive model.  

One strategy for variable selection, if the total number of variables is say less than 10 or 15, would be to run an Multiple Linear Regression (for predicting a continuous outcome) or a Generalized Linear Model (for predicting a binary response) on all the variables and then to run it again on a single variable. Note the difference in the performance - which performed better? If the single variable model performed better, then perhaps an increase in variables, up to a certain point, will improve the performance of the model. At this point, I would use a Forward or Backward Step selection process and compare these results to other linear models with variables selected by the user (i.e. me), including some interaction and second degree terms. Again, compare these results to each other as well as to the initial results. Is there a pattern in the results? Is there a "feeling" that the results can be improved upon? Or does one of the initial results (single variable or all variables) still the best one?  

It's difficult to say when the variable selection process is complete. It depends on not only the selection process, but also the performance of the model on the test set. How useful is the final model in predicting the desired outcomes? A model is only as good as its correct predictions. So even a model that fitted and tested well will still need to be routinely assessed in regards to it's efficacy in its application.
